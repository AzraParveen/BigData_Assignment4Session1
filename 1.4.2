possible solutions to handle big data are :Scale up and Scale out

i. Scale up: where this is not a recommended one which the data storage exceeds the limit
   the storage device capacity can be added up.as well it a costly,time consuming and a 
   complex process.
> Increase the configuration of a single system, like disk capacity, RAM, data transfer
  speed, etc.
> Complex, costly, and a time consuming process

ii. Scale out: where here economical machines can be added up instead of adding the data storage
through devices.many number of computers can be added together to share the data's.and this is 
the better method than scale up.which is economical as well work is distributed among different
machines.
> Use multiple commodity (economical) machines and distribute the load of
  storage/processing among them
> Economical and quick to implement as it focuses on distribution of load
> Instead of having a single system with 10 TB of storage and 80 GB of RAM, use 40
  machines with 256 GB of storage and 2 GB of RAM


Scale-up -> can solve a capacity problem without adding infrastructure elements such as network 
connectivity. However, it does require additional space, power, and cooling. Scaling up does 
not add controller capabilities to handle additional host activities. That means it doesn’t 
add costs for extra control functions either.
So the costs have not scaled at the same rate for the initial storage system plus storage 
devices – only additional devices have been added.
 
Scale-out -> storage usually requires additional storage (called nodes) to add capacity and 
performance. Or in the case of monolithic storage systems, it scales by adding more functional
elements (usually controller cards).One difference between scaling out and just putting more
storage systems on the floor is that scale-out storage continues to be represented as a single system

